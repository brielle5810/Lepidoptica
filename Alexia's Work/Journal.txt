Previously:
- Decided to try to make my own model to recognize text. I'm attempting to follow a research paper (very roughly because the paper's model was created with Matlab) and I have a very large dataset of text and other information. 
- A large portion of my time was spent trying to figure out how to import all the data and organize it. 

2/21/25 Update:
Spent a lot of time reorganizing preprocessing, reading/watching resources on what is the best ways to preprocess for ocr, and trying out different methods to see what gives the best results. I found that any form of noise reduction, whether it be CLAHE, Dialating, Eroding, MorphologyEx, etc would make my images much more busy and hard to read.

I also read an article about Adaptive Thresholding, which is much more useful for images of variable lighting (which is how my data is). Adaptive Thresholding determines the threshold of a pixel based on the region around it, which allows binarization to be more uniform. I used Adaptive Gaussian Thresholding. It worked a lot better than my previous attempts with Otsu's Binarization.

Other resources I want to look into:

https://www.geeksforgeeks.org/bounding-box-prediction-using-pytorch/
https://builtin.com/data-science/python-ocr
https://www.youtube.com/watch?v=9FCw1xo_s0I
https://github.com/wjbmattingly/ocr_python_textbook/blob/main/03_bounding_boxes_index-Copy1.ipynb
https://www.reddit.com/r/deeplearning/comments/sxagv0/help_training_a_model_on_google_colab_and_my/

Update: 
Hit issues with RAM on Google Colab. I'm still holding out for Hypergator, so I don't want to buy Google Colab pro. Though as it's been already a few weeks since they told us they'd give us access, I'm starting to think I should buy it anyways.

Update:
Found a work around the RAM issue, but now just had issues training the model. My runtime disconnects before the model is finished training (which would be like... 4 days). Essentially it means that it never completes. So I had to buy Google Colab pro.

3/10/25 Update:
Last week, I finally successfully was able to run the model. Over the weekend, I tested two different runs successfully, but the best the model ever did in accuracy was 32%. It hit that percentage after only a few epochs, making me think that the model is overfitting. That would make sense, since I realized that the images I had in my data file (though incredibly diverse) was also super busy and hard to deal with. Each image had extra items in it, people, varying levels of readability, etc. I think it was a foolish idea to try and make a model out of this data, especially since the butterfly images we would've been working with would be more standard and similar to one another. But I guess I was hoping to get a good grasp on both handwritten and machine text. 

Instead, I'm going to look at pytesseract. We don't have enough time to go through what I just did and try to create our own model, so I'll see if pytesseract can make our lives easier. I can then work on just trying to build on the pytesseract model, like my teammates have been doing. 

Later 3/10/25 Update:
I decided to try easyocr after watching a few resources. It's terrible on non processed images, but after preprocessing, it worked out pretty good (not too bad either with some more legible handwriting). Even better, we get a confidence percentage on how good each text prediction is. That way, we can tell the user when the machine thinks it's made a mistake and the user can go in and fix it themselves. Next time, I'm going to see if I can improve the accuracy and test it on multiple images. 

3/11/25 Update: 
I tweaked the preprocessing best I could, I think I got the best results right now assuming the image has legible text. I also drafted what the website may look like, but that's pending approval from the team.

3/27/2025 Update:
Over this week, I followed a few tutorials to try and figure out how to fine tune easyocr. The tutorials are listed below:
- https://pub.towardsai.net/how-to-fine-tune-easyocr-to-achieve-better-ocr-performance-1540f5076428
- https://www.freecodecamp.org/news/how-to-fine-tune-easyocr-with-a-synthetic-dataset/#heading-how-to-install-required-packages
- https://drive.google.com/drive/folders/1rS-WFRqN9zkD3vetwcYYFmOzg_cMv9su
- https://github.com/JaidedAI/EasyOCR/tree/master/trainer
- https://github.com/JaidedAI/EasyOCR/blob/master/custom_model.md
- https://github.com/clovaai/deep-text-recognition-benchmark

I was first following the top two tutorials, which meant pulling the deep-text-recognition-benchmark GitHub and making a lmdb format dataset. In the process, I began to understand easyocr's trainer. I scraped all the work I did there and tried instead pulling easyocr's GitHub and merging all required files with what is included in the deep-text-recognition-benchmark (I wasn't sure if easyocr had everything I needed,  or if I had to put both of these files together, so I just merged them. Whenever there was a crossroad, I prioritized easyocr). I was able to get the program to run finally. It took about a day and a few hours before it had an error. 

In the effort to get more of the program done, I'm pausing my work here and instead trying to integrate easyocr functionality into the program. Once I'm done with this, I'm going to go back and try fine-tuning stuff again. I think I know what the issue I had was about: I need to replace all the changes I made in train.py with the original easyocr code. It should be ok then. I also want to see if it's possible to run this code on google drive/google colab. That would make things a little easier for me time wise. 

4/2/2025 Update:
I was able to get fine-tuning to work on google colab! I'm using the standard character set though and a very small amount of data, so seeing improvements is like non existent. No matter, I'm just going to go ahead and use data from the nanonets my team worked on. 

4/9/2025 Update: 
Last Monday, I took all the nanonets that my team was working on and made a program to reformat and split the data into test and train. I had about 4000 images (I think). I also updated the character list to include the sex symbols, degree sign, and a few more special characters not typically used in English but used in Spanish or other languages (they existed in names). I ran into a week long session of debugging. Most of the issue was the new character sheet I uploaded. The original model was expecting 97 classes (a class for each character). I had just risen that number to 120. I had to go through and make sure the model wasn't expecting that original shape and to instead reform the original model to fit more classes. Then, I had a problem with CUDA. I'm honestly not sure what happened here, but I had to disable it right before CTC_loss cost was calculated in testing. Otherwise, the model would just abruptly fail. Once I did that though, the model was gradually running (over the course of 8 hours)! It made it through 30,000 iterations, the intended number of iterations. Easyocr's training files would automatically save the model with the best accuracy and norm_ED or whatever its called, and that was super helpful. 

Once I had these models, I used it in combination with a yaml and py file (edited from the provided examples) to use a fine-tuned model. When I incorporated it into our flask website, my team noticed a considerable amount of quotation marks. I realized that for some reason, after reading in the labels csv, it would wrap text around in quotes if there was a comma included within. I added a line to combat that and remade the model.

I have fine tuning in a good place, I honestly don't *need* to do more, but I want to. I'm at 73% accuracy right now but with more nanonet data, I can fine tune the fine tuned model and get even better (at least that's what I'm hoping). I'm going to pause this though and focus on redoing the reprocessing page (just because it needs a glow up). Once I'm done with that, I'll work harder on nanonets and then retrain the model. 